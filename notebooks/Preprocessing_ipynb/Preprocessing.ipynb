{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fc1b08-3027-4ee3-890f-94bc9a340b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory for top level folder\n",
    "dir_ = \"/home/sugam/Work/20-29 Deep Learning/22 Projects/Optimization of Energy Using AIML/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef710429-6851-4aac-b0e1-64778c010be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = dir_ + 'raw/'\n",
    "processed_data_dir = dir_+'Processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d81b8c-30c9-499d-b610-44c70163dfc4",
   "metadata": {},
   "source": [
    "# 1. MAIN SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21b540-aa31-4274-8b06-320a9c2aac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genderal imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc,time,warnings,pickle,psutil,random\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "import mplcatppuccin\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use([\"ggplot\", \"mocha\"])\n",
    "plt.rcParams[\"figure.figsize\"] = (25,8)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3403cf-2d1e-465d-9f47-6e05be073a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# MEMORY PROFILER\n",
    "## Displays memory used by dataframe\n",
    "\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812207c8-07fd-4a8f-9e6d-cfe0f19c3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## MEMORY REDUCER\n",
    "## Function which checks each column and manage the dtype automatically\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print(f'Mem. usage decreased to {end_mem:5.2f} Mb {(100 * (start_mem - end_mem) / start_mem):.1f}% reduction')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56aa68-30d0-4ef8-8c93-d8c629ce7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1,df2,merge_on):\n",
    "    merged_df = df1[merge_on] # merge_on is multiple columns\n",
    "    merged_df = merged_df.merge(df2,on=merge_on,how='left')\n",
    "    new_columns = [col for col in list(merged_df) if col not in merge_on]\n",
    "    df1 = pd.concat([df1,merged_df[new_columns]],axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758eef2-d942-47b9-944d-a0f6ef4e8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Vars\n",
    "###################################################################\n",
    "TARGET = [\"load\"] # main target -> Going to combine every load to a single variable\n",
    "END_TRAIN = '2020-12-04 23:45:00' # Last day of train set\n",
    "MAIN_INDEX = ['date'] # Identify each entry by these columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81b0cc-c1f0-46b4-be7d-0bea5e9594f5",
   "metadata": {},
   "source": [
    "# 2. PART -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de8ca2-b6a6-4136-85a8-8f5e716218f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### LOAD DATA\n",
    "##################################################################\n",
    "def load_data(url: str) ->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the dataframe into memory\n",
    "\n",
    "    Args:\n",
    "      url (string) - the path to the dataframe\n",
    "\n",
    "    Returns:\n",
    "      df (pandas dataframe)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"⏳ Load Data\")\n",
    "    \n",
    "    # Refering our data without any modification and dtype\n",
    "    df = pd.read_csv(url,\n",
    "                           index_col=0,\n",
    "                           parse_dates = True)\n",
    "\n",
    "    for col in list(df):  # List out the column names\n",
    "        if \"Unnamed\" in col:  # Checks if the column contains Unnamed in it \n",
    "            df.drop(col, axis=1,inplace=True) # Removes the Garbage column\n",
    "    \n",
    "    #df.drop_duplicates(subset='date',inplace=True)\n",
    "\n",
    "    df.index = pd.to_datetime(df.index,format='mixed',dayfirst=True) # Converting index to date time format\n",
    "    \n",
    "    print(\"✅ Load Data\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df_dir = raw_data_dir+\"01 Energy Usage Data/energy_usage.csv\"\n",
    "weather_df_dir = raw_data_dir+\"02 Outdoor Environmental Data/site_weather.csv\"\n",
    "\n",
    "train_df = load_data(train_df_dir)\n",
    "weather_df = load_data(weather_df_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b32f2-0715-47f6-bbd7-88066fa36ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_columns(train_df: pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a main dataframe and adds up all the columns to make a single load column\n",
    "\n",
    "    Args:\n",
    "      train_df (pandas dataframe) - Original dataframe\n",
    "\n",
    "    Returns:\n",
    "      train_df (pandas dataframe) - dataframe with a single load column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combining all North wings and South wing load into a single variables\n",
    "    train_df[\"load\"] = (train_df['Mis_elect_load_south']+\n",
    "                           train_df['light_load_south']+\n",
    "                           train_df['hvac_load_south']+\n",
    "                           train_df['Mis_elect_load_north']+\n",
    "                           train_df['hvac_load_north']\n",
    "                          )\n",
    "    \n",
    "    train_df.drop(['Mis_elect_load_south',\n",
    "                   'light_load_south',\n",
    "                   'hvac_load_south',\n",
    "                   'Mis_elect_load_north',\n",
    "                   'hvac_load_north']\n",
    "                  ,axis=1,\n",
    "                 inplace=True)\n",
    "\n",
    "    return train_df\n",
    "    \n",
    "\n",
    "train_df = add_all_columns(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a693c2b-cdb1-42eb-946d-e2bb1310d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"load\"].plot()\n",
    "plt.ylabel(\"Load\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caadece-32fc-40a7-b4e8-d258263a9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at 1 week worth of data from June-01-2018 to June-07-2018\n",
    "train_df.loc[\"2018-06-01\":\"2018-06-07\"].plot()\n",
    "plt.title(\"June-1-2018 to June-7-2018\");\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0353d-7b67-415b-bdc5-38a1c07a42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_date_column(df):\n",
    "    \"\"\"\n",
    "    Takes the dataframe and creates columns like year, month, day, hour and minute from datetime index\n",
    "\n",
    "    Args:\n",
    "      df (pandas dataframe) - original dataframe\n",
    "\n",
    "    Returns:\n",
    "      df (pandas dataframe) - dataframe with added columns\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"year\"] = df.index.year\n",
    "    df[\"month\"] = df.index.month.astype(np.int8)\n",
    "    df[\"day\"] = df.index.day.astype(np.int8)\n",
    "    df[\"hour\"] = df.index.hour.astype(np.int8)\n",
    "    df[\"minute\"] = df.index.minute.astype(np.int8)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d3b0c-f223-44ab-b2a5-f953a6f519ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns(df):\n",
    "    \"\"\"\n",
    "    Takes the dataframe and adds the extra column\n",
    "    Args:\n",
    "      df (pandas dataframe) - Original dataframe\n",
    "\n",
    "    Return:\n",
    "      df (pandas dataframe) - dataframe with added columns\n",
    "    \"\"\"\n",
    "    # Monday = 0 , Sunday = 6\n",
    "    df[\"weekday\"] = train_df.index.weekday.astype(np.int8)\n",
    "    \n",
    "    \n",
    "    # This adds number of week in a month\n",
    "    df[\"week_num\"] = df[\"day\"].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "    # Friday saturday and sunday are weekends = 1 , everything else is 0\n",
    "    df[\"is_weekdend\"] = df.index.weekday.map(lambda x: 1 if x>=5 else 0)\n",
    "    \n",
    "    df[\"load_min\"] = df.groupby([\"year\",\"month\",\"week_num\"])[\"load\"].transform(\"min\")\n",
    "    df[\"load_max\"] = df.groupby([\"year\",\"month\",\"week_num\"])[\"load\"].transform(\"max\")\n",
    "    df[\"load_mean\"] = df.groupby([\"year\",\"month\",\"week_num\"])[\"load\"].transform(\"mean\")\n",
    "    df[\"load_std\"] = df.groupby([\"year\",\"month\",\"week_num\"])[\"load\"].transform(\"std\")\n",
    "    # Normalize the energy (min/max scaling)\n",
    "    df[\"load_norm\"] = df[\"load\"] / (df[\"load_max\"]-df[\"load_min\"]) \n",
    "        \n",
    "    return df\n",
    "\n",
    "train_df = create_basic_date_column(train_df)\n",
    "train_df = create_columns(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d865ca-c8c1-46b0-b061-919314e7f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"load_min\",\"load_max\",\"load_mean\",\"load_std\",\"load_norm\"]].plot(subplots=True,figsize=(25,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff132348-4db3-481b-b37f-e1d04ee53441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True,inplace=True) # Drop the date index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98b414-d000-49f6-a831-7a8e4bb56a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na(df: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks for the null values if found then fills it wil the average load value of the particular month\n",
    "\n",
    "    Args:\n",
    "      df (pandas dataframe) - the original dataframe\n",
    "\n",
    "    Returns:\n",
    "      df (pandas dataframe) - dataframe with na values filled with the average values from that particular month\n",
    "    \n",
    "    \"\"\"\n",
    "    is_null = len(df.isnull().sum(axis=0)[df.isnull().sum(axis=0)>0]) > 0 \n",
    "    if is_null:\n",
    "        month_wise_mean = df.groupby([\"year\",\"month\"])[\"load\"].transform(\"mean\")\n",
    "        df[\"load\"].fillna(month_wise_mean, inplace=True)\n",
    "        del month_wise_mean\n",
    "    df.fillna(method = \"bfill\",inplace=True)   \n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = remove_na(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22e9ed-e0f9-45ed-b6b5-d6734a3c19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(method=\"ffill\",inplace=True)\n",
    "train_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552fc27-1617-4daf-883d-51b66e9d7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(train_df[\"load\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1fe17-d845-46f5-80dd-cfea77ad509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the size of the dataframe\n",
    "print(\"{}: {}\".format('Original',sizeof_fmt(train_df.memory_usage(index=True).sum())))\n",
    "\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "\n",
    "print(\"{}: {}\".format('After reducing',sizeof_fmt(train_df.memory_usage(index=True).sum())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999f58b-444c-45ac-8183-7332a6611651",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Save the Dataframe\n",
    "print(\"✅ Save part 1\")\n",
    "train_df.to_pickle(processed_data_dir+\"train_df_part_1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca9b5e-9841-4889-88fb-f1a719d46197",
   "metadata": {},
   "source": [
    "# 3. Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1323f9-3f67-4d80-ab97-8786d5e661ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = load_data(weather_df_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df7e67-9b81-474b-b28f-39b9e3ce484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df[\"dew_point_temperature_sensor_2\"] = weather_df[\"dew_point_temperature_sensor_2\"].replace(\"`\",np.nan)\n",
    "weather_df.fillna(method = 'bfill',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f445ec0-571a-4269-a2b8-ad2c45b29b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = create_basic_date_column(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ffae0-9dc3-4467-a3b7-cd918f298a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cols = [\"air_temp_sensor_1\",\"air_temp_sensor_2\",\"dew_point_temperature_sensor_2\",\"relative_humidity_sensor_1\",\"solar_radiation_sensor_1\"] \n",
    "weather_df[main_cols].plot(subplots=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fce655-e6f1-4ac6-b6a5-e88dd66ad7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_columns(df):\n",
    "    \n",
    "    df[\"avg_air_temp\"] = weather_df[[\"air_temp_sensor_1\",\"air_temp_sensor_2\"]].mean(axis=1)\n",
    "\n",
    "    df.drop([\"air_temp_sensor_1\",\"air_temp_sensor_2\"],axis=1,inplace=True)\n",
    "    \n",
    "    df.reset_index(drop=True,inplace=True) # Removing the date index\n",
    "    \n",
    "    df.rename(columns={\"dew_point_temperature_sensor_2\": \"dew_point_temp\",\n",
    "                      \"relative_humidity_sensor_1\": \"relative_humidity\",\n",
    "                      \"solar_radiation_sensor_1\": \"solar_radiation\"},\n",
    "                     inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "weather_df = preprocess_columns(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35176c9-79f5-4578-ae2d-8f6b9508cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cols = [\"avg_air_temp\",\"dew_point_temp\",\"relative_humidity\",\"solar_radiation\"] \n",
    "weather_df[process_cols].plot(subplots=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94432a7-a879-4206-a494-84a60eca9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = reduce_mem_usage(weather_df)\n",
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841fbb9-83c0-4674-9098-60d58d77d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################3 Merge and save to part 2\n",
    "print(\"Merge Weather and save it for part 2\")\n",
    "\n",
    "merge_on = [\"year\",\"month\",\"day\",\"hour\",\"minute\"]\n",
    "train_df = train_df.merge(weather_df,\n",
    "                          on =merge_on,\n",
    "                          how=\"left\")\n",
    "\n",
    "print(\"✅ Save part 2\")\n",
    "train_df.to_pickle(processed_data_dir+\"train_df_part_2.pkl\")\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48d183-514a-4332-9126-9084527a4ab4",
   "metadata": {},
   "source": [
    "# Lag Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c442deb-8d4d-4f1d-8431-a0db35d0808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(processed_data_dir+\"train_df_part_1.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23c179-c8dc-4f4e-9ab4-8fc4664a5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need year, month, day , hour , minute and load\n",
    "df = df[[\"year\",\"month\",\"day\",\"hour\",\"minute\",\"load\"]]\n",
    "SHIFT_DAY = 1\n",
    "\n",
    "# Lags with 1 day\n",
    "start_time = time.time()\n",
    "print(\"Create Lags\")\n",
    "\n",
    "LAG_DAYS = [col for col in range(SHIFT_DAY, SHIFT_DAY+15)]\n",
    "df = df.assign(**{\n",
    "    \"{}_lag_{}\".format(col,l): df[col].shift(l)\n",
    "    for l in LAG_DAYS\n",
    "    for col in TARGET\n",
    "})\n",
    "print('%0.2f minute: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6b3c5-a5d8-4fdd-992f-e75636b15643",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    if \"lag\" in col:\n",
    "        df[col] = df[col].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e606dd8-c99f-4563-bc11-4ebe24c8863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print('Create rolling aggs')\n",
    "\n",
    "for i in [7,14,30,60,180]:\n",
    "    print('Rolling period:', i)\n",
    "    df['rolling_mean_'+str(i)] = df[TARGET].shift(SHIFT_DAY).rolling(i).mean().astype(np.float16)\n",
    "    df['rolling_std_'+str(i)]  = df[TARGET].shift(SHIFT_DAY).rolling(i).std().astype(np.float16)\n",
    "\n",
    "# Rollings\n",
    "# with sliding shift\n",
    "for d_shift in [1,7,14]: \n",
    "    print('Shifting period:', d_shift)\n",
    "    for d_window in [7,14,30,60]:\n",
    "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
    "        df[col_name] = df[TARGET].shift(d_shift).rolling(d_window).mean().astype(np.float16)\n",
    "    \n",
    "    \n",
    "print('%0.2f minute: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5828c-645c-4fb5-a6f6-36046ed64917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42763ae7-8815-4c0c-bd58-2caf0aeaab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "print('Save lags and rollings')\n",
    "df.to_pickle(processed_data_dir+'lags_df_'+str(SHIFT_DAY)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de020276-1ca4-4660-a51f-5f080be891f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Combine train_df_1 and Lags\n",
    "train_df_part_1 = pd.read_pickle(processed_data_dir+\"train_df_part_1.pkl\")\n",
    "\n",
    "# Combined the original energy df -> train_df_part_1 and the df with lag features\n",
    "# Storing the combined form into train_df_energy\n",
    "train_df_energy = train_df_part_1.merge(df,on=[\"year\",\"month\",\"day\",\"hour\",\"minute\"],how = \"left\")\n",
    "train_df_energy.rename({\"load_x\":\"load\"},inplace=True)\n",
    "train_df_energy.drop(\"load_y\",axis=1,inplace=True)\n",
    "\n",
    "print(\"Saving final Energy df\")\n",
    "train_df_energy.to_pickle(processed_data_dir+\"train_df_energy_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c794925-9605-49f2-8532-2f6a0c3988c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb8edc-25ef-415b-ab56-af787928974a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
