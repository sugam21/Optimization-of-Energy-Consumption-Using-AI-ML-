{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fc1b08-3027-4ee3-890f-94bc9a340b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory for top level folder\n",
    "dir_ = \"/home/sugam/Work/20-29 Deep Learning/22 Projects/M5-Forecasting/m5-forecasting-accuracy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef710429-6851-4aac-b0e1-64778c010be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = dir_ + '2.data/'\n",
    "processed_data_dir = dir_+'2.data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d81b8c-30c9-499d-b610-44c70163dfc4",
   "metadata": {},
   "source": [
    "# 1. MAIN SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb21b540-aa31-4274-8b06-320a9c2aac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genderal imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc,time,warnings,pickle,psutil,random\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3403cf-2d1e-465d-9f47-6e05be073a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple memory profilers to see memeory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812207c8-07fd-4a8f-9e6d-cfe0f19c3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a56aa68-30d0-4ef8-8c93-d8c629ce7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1,df2,merge_on):\n",
    "    merged_df = df1[merge_on] # merge_on is multiple columns\n",
    "    merged_df = merged_df.merge(df2,on=merge_on,how='left')\n",
    "    new_columns = [col for col in list(merged_df) if col not in merge_on]\n",
    "    df1 = pd.concat([df1,merged_df[new_columns]],axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4758eef2-d942-47b9-944d-a0f6ef4e8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Vars\n",
    "###################################################################\n",
    "TARGET = 'sales' # main target\n",
    "END_TRAIN = 1941 # Last day of train set\n",
    "MAIN_INDEX = ['id','d'] # Identify each entry by these columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81b0cc-c1f0-46b4-be7d-0bea5e9594f5",
   "metadata": {},
   "source": [
    "# 2. PART -1 \n",
    "- Melting train data => grid_part_1\n",
    "- creating price features => grid_part_2\n",
    "- creating calender features => grid_part_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1de8ca2-b6a6-4136-85a8-8f5e716218f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "####################### LOAD DATA\n",
    "##################################################################\n",
    "print(\"Load Main Data\")\n",
    "# Refering our data without any modification and dtype\n",
    "train_df = pd.read_csv(raw_data_dir+'sales_train_evaluation.csv')\n",
    "prices_df = pd.read_csv(raw_data_dir+'sell_prices.csv')\n",
    "calendar_df = pd.read_csv(raw_data_dir+'calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb24a5c4-a5c4-422a-a276-700cff598450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "Train rows: 30490 --- 59181090\n",
      "    Original grid_df:   4.0GiB\n",
      "     Reduced grid_df:   1.8GiB\n"
     ]
    }
   ],
   "source": [
    "####################### MAKE GRID\n",
    "##################################################################\n",
    "print(\"Create Grid\")\n",
    "\n",
    "# We are unpivoting the table inorder to convert wide format table into long format\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "grid_df = pd.melt(train_df,\n",
    "                 id_vars = index_columns,\n",
    "                 var_name = 'd',\n",
    "                 value_name = TARGET)\n",
    "# In the train_df, there are very few training rows\n",
    "# But each day can provide a lot of training data\n",
    "print(f\"Train rows: {len(train_df)} --- {len(grid_df)}\")\n",
    "\n",
    "# To be able to make predictions we need to add test set to our grid\n",
    "# Below code adds the new rows for future data.\n",
    "# It will add 28 days in the future\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates() # Ensures data is unique which you going to predict in the future\n",
    "    temp_df['d'] = \"d_\"+str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df.reset_index(drop=False,inplace=True)\n",
    "del temp_df, add_grid\n",
    "del train_df\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "# We can free some memory \n",
    "# by converting \"strings\" to categorical\n",
    "# it will not affect merging and \n",
    "# we will not lose any valuable data\n",
    "for col in index_columns:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eceeefe-9ca2-43e9-8e68-7ba609ca1521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n",
      "    Original grid_df:   2.1GiB\n",
      "     Reduced grid_df:   1.9GiB\n"
     ]
    }
   ],
   "source": [
    "########################### Product Release date\n",
    "#################################################################################\n",
    "print('Release week')\n",
    "\n",
    "# It seems that leadings zero values\n",
    "# in each train_df item row\n",
    "# are not real 0 sales but mean\n",
    "# absence for the item in the store\n",
    "# we can safe some memory by removing\n",
    "# such zeros\n",
    "\n",
    "# Prices are set by week\n",
    "# so it we will have not very accurate release week \n",
    "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "release_df.columns = ['store_id','item_id','release']\n",
    "\n",
    "# Now we can merge release_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
    "del release_df\n",
    "\n",
    "# We want to remove some \"zeros\" rows\n",
    "# from grid_df \n",
    "# to do it we need wm_yr_wk column\n",
    "# let's merge partly calendar_df to have it\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
    "                      \n",
    "# Now we can cutoff some rows \n",
    "# and safe memory \n",
    "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Should we keep release week \n",
    "# as one of the features?\n",
    "# Only good CV can give the answer.\n",
    "# Let's minify the release values.\n",
    "# Min transformation will not help here \n",
    "# as int16 -> Integer (-32768 to 32767)\n",
    "# and our grid_df['release'].max() serves for int16\n",
    "# but we have have an idea how to transform other columns in case we will need it\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbe192e-3961-4156-9a19-1e961f92717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Part 1\n",
      "Size: (47735397, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################### Save part 1\n",
    "#################################################################################\n",
    "print('Save Part 1')\n",
    "\n",
    "# We have our BASE grid ready\n",
    "# and can save it as pickle file\n",
    "# for future use (model training)\n",
    "grid_df.to_pickle(processed_data_dir+'grid_part_1.pkl')\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e553c0c-5cbe-4e06-98bc-2c17aec7799f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_id        item_id  wm_yr_wk  sell_price\n",
       "0     CA_1  HOBBIES_1_001     11325        9.58\n",
       "1     CA_1  HOBBIES_1_001     11326        9.58\n",
       "2     CA_1  HOBBIES_1_001     11327        8.26\n",
       "3     CA_1  HOBBIES_1_001     11328        8.26\n",
       "4     CA_1  HOBBIES_1_001     11329        8.26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa829f33-2d57-42d5-b779-c89cd3895f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices\n"
     ]
    }
   ],
   "source": [
    "########################## PRICES\n",
    "######################################################\n",
    "print(\"Prices\")\n",
    "\n",
    "# We can do some basic aggregration\n",
    "prices_df['price_max'] = prices_df.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].transform(\"max\") # this gives you the maximum price each unique product has been sold to .\n",
    "# Same product can be sold to many prices but the above code finds out the maximum price. Finally it replaces each item in that group with the maximum value.\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df[\"price_mean\"] = prices_df.groupby([\"store_id\",\"item_id\"])['sell_price'].transform(\"mean\")\n",
    "prices_df[\"price_std\"] = prices_df.groupby([\"store_id\",\"item_id\"])['sell_price'].transform(\"std\")\n",
    "\n",
    "\n",
    "# Doing price normalization(min/max scaling)\n",
    "prices_df[\"price_norm\"] = prices_df[\"sell_price\"]/prices_df[\"price_max\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb8dc39c-6cc6-44f1-b5e9-f215328f4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some items can be inflation dependent and some can be stable\n",
    "prices_df[\"price_nunique\"] = prices_df.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].transform(\"nunique\")\n",
    "prices_df[\"item_nuinque\"] = prices_df.groupby([\"store_id\",\"item_id\"])[\"item_id\"].transform(\"nunique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92d4b723-044e-4bb5-addb-38a9f66cc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making rolling aggregations but with months and years as window\n",
    "calendar_prices = calendar_df[[\"wm_yr_wk\",\"month\",\"year\"]]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=[\"wm_yr_wk\"])\n",
    "prices_df = prices_df.merge(calendar_prices[[\"wm_yr_wk\",\"month\",\"year\"]],on=[\"wm_yr_wk\"],how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "103a2d72-c6ca-46d9-a1ed-a9d80f7f91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del calendar_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d5e46a1-be30-4337-8759-106b8daf2213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>price_max</th>\n",
       "      <th>price_min</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>price_std</th>\n",
       "      <th>price_norm</th>\n",
       "      <th>price_nunique</th>\n",
       "      <th>item_nuinque</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "      <td>9.58</td>\n",
       "      <td>8.26</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "      <td>9.58</td>\n",
       "      <td>8.26</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "      <td>9.58</td>\n",
       "      <td>8.26</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>0.862213</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "      <td>9.58</td>\n",
       "      <td>8.26</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>0.862213</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "      <td>9.58</td>\n",
       "      <td>8.26</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>0.862213</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841116</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11617</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841117</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11618</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841118</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11619</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841119</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11620</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841120</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>11621</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6841121 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        store_id        item_id  wm_yr_wk  sell_price  price_max  price_min  \\\n",
       "0           CA_1  HOBBIES_1_001     11325        9.58       9.58       8.26   \n",
       "1           CA_1  HOBBIES_1_001     11326        9.58       9.58       8.26   \n",
       "2           CA_1  HOBBIES_1_001     11327        8.26       9.58       8.26   \n",
       "3           CA_1  HOBBIES_1_001     11328        8.26       9.58       8.26   \n",
       "4           CA_1  HOBBIES_1_001     11329        8.26       9.58       8.26   \n",
       "...          ...            ...       ...         ...        ...        ...   \n",
       "6841116     WI_3    FOODS_3_827     11617        1.00       1.00       1.00   \n",
       "6841117     WI_3    FOODS_3_827     11618        1.00       1.00       1.00   \n",
       "6841118     WI_3    FOODS_3_827     11619        1.00       1.00       1.00   \n",
       "6841119     WI_3    FOODS_3_827     11620        1.00       1.00       1.00   \n",
       "6841120     WI_3    FOODS_3_827     11621        1.00       1.00       1.00   \n",
       "\n",
       "         price_mean  price_std  price_norm  price_nunique  item_nuinque  \\\n",
       "0          8.285714   0.152139    1.000000              3             1   \n",
       "1          8.285714   0.152139    1.000000              3             1   \n",
       "2          8.285714   0.152139    0.862213              3             1   \n",
       "3          8.285714   0.152139    0.862213              3             1   \n",
       "4          8.285714   0.152139    0.862213              3             1   \n",
       "...             ...        ...         ...            ...           ...   \n",
       "6841116    1.000000   0.000000    1.000000              1             1   \n",
       "6841117    1.000000   0.000000    1.000000              1             1   \n",
       "6841118    1.000000   0.000000    1.000000              1             1   \n",
       "6841119    1.000000   0.000000    1.000000              1             1   \n",
       "6841120    1.000000   0.000000    1.000000              1             1   \n",
       "\n",
       "         month  year  \n",
       "0            7  2013  \n",
       "1            7  2013  \n",
       "2            7  2013  \n",
       "3            8  2013  \n",
       "4            8  2013  \n",
       "...        ...   ...  \n",
       "6841116      5  2016  \n",
       "6841117      5  2016  \n",
       "6841118      6  2016  \n",
       "6841119      6  2016  \n",
       "6841120      6  2016  \n",
       "\n",
       "[6841121 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce54bdf5-efe0-491e-97fa-a7ace9febec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8907b5-8661-4705-a544-6c3c866361b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
